@article{Hochkirchen2010,
   abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a ~without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
   author = {Thomas Hochkirchen},
   doi = {10.1111/j.1467-985x.2009.00634_10.x},
   issn = {09641998},
   issue = {2},
   journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
   title = {Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning},
   volume = {173},
   year = {2010},
}
@article{23,
   abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
   author = {Sepp Hochreiter and Jürgen Schmidhuber},
   doi = {10.1162/neco.1997.9.8.1735},
   issn = {08997667},
   issue = {8},
   journal = {Neural Computation},
   title = {Long Short-Term Memory},
   volume = {9},
   year = {1997},
}
@article{Lange1997,
   abstract = {This book provides a solid statistical foundation for neural networks from a pattern recognition perspective. The focus is on the types of neural nets that are most widely used in practical applications, such as the multi-layer perceptron and radial basis function networks. Rather than trying to cover many different types of neural networks, Bishop thoroughly covers topics such as density estimation, error functions, parameter optimization algorithms, data pre-processing, and Bayesian methods. All topics are organized well and all mathematical foundations are explained before being applied to neural networks. The text is suitable for a graduate or advanced undergraduate level course on neural networks or for practitioners interested in applying neural networks to real-world problems. The reader is assumed to have the level of math knowledge necessary for an undergraduate science degree. This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.},
   author = {Nicholas Lange and C. M. Bishop and B. D. Ripley},
   doi = {10.2307/2965437},
   issn = {01621459},
   issue = {440},
   journal = {Journal of the American Statistical Association},
   title = {Neural Networks for Pattern Recognition.},
   volume = {92},
   year = {1997},
}
@article{Bedi2019,
   abstract = {The increasing world population and availability of energy hungry smart devices are major reasons for alarmingly high electricity consumption in the current times. So far, various simulation tools, engineering and Artificial Intelligence based methods are being used to perform optimal electricity demand forecasting. While engineering methods use dynamic equations to forecast, the AI-based methods use historical data to predict future demand. However, modeling of nonlinear electricity demand patterns is still underdeveloped for robust solutions as the existing methods are useful only for handling short-term dependencies. Moreover, the existing methods are static in nature because they are purely historical data driven. In this paper, we propose a deep learning based framework to forecast electricity demand by taking care of long-term historical dependencies. Initially, the cluster analysis is performed on the electricity consumption data of all months to generate season based segmented data. Subsequently, load trend characterization is carried out to have a deeper insight of metadata falling into each of the clusters. Further, Long Short Term Memory network multi-input multi-output models are trained to forecast electricity demand based upon the season, day and interval data. In the present work, we have also incorporated the concept of moving window based active learning to improve prediction results. To demonstrate the applicability and effectiveness of the proposed approach, it is applied to the electricity consumption data of Union Territory Chandigarh, India. Performance of the proposed approach is evaluated by comparing the prediction results with Artificial Neural Network, Recurrent Neural Network and Support Vector Regression models.},
   author = {Jatin Bedi and Durga Toshniwal},
   doi = {10.1016/J.APENERGY.2019.01.113},
   issn = {0306-2619},
   journal = {Applied Energy},
   keywords = {Active forecasting,Electricity demand prediction,Energy analytic,LSTM network,Recurrent neural network},
   month = {3},
   pages = {1312-1326},
   publisher = {Elsevier},
   title = {Deep learning framework to forecast electricity demand},
   volume = {238},
   year = {2019},
}
@article{1,
   abstract = {Forecasting electricity demand through time series is a tool used by transmission system operators to establish future operating conditions. The accuracy of these forecasts is essential for the precise development of activity. However, the accuracy of the forecasts is enormously subject to the calendar effect. The multiple seasonal Holt-Winters models are widely used due to the great precision and simplicity that they offer. Usually, these models relate this calendar effect to external variables that contribute to modification of their forecasts a posteriori. In this work, a new point of view is presented, where the calendar effect constitutes a built-in part of the Holt-Winters model. In particular, the proposed model incorporates discrete-interval moving seasonalities. Moreover, a clear example of the application of this methodology to situations that are difficult to treat, such as the days of Easter, is presented. The results show that the proposed model performs well, outperforming the regular Holt-Winters model and other methods such as artificial neural networks and Exponential Smoothing State Space Model with Box-Cox Transformation, ARMA Errors, Trend and Seasonal Components (TBATS) methods.},
   author = {Óscar Trull and J. Carlos García-Díaz and Alicia Troncoso},
   doi = {10.3390/en12061083},
   issn = {19961073},
   issue = {6},
   journal = {Energies},
   title = {Application of discrete-interval moving seasonalities to Spanish electricity demand forecasting during easter},
   volume = {12},
   year = {2019},
}
@inproceedings{2,
   abstract = {In recent years the available volume of information has grown considerably due to the development of new technologies such as the sensor networks or smart meters, and therefore, new algorithms able to deal with big data are necessary. In this work the distributed version of the k-means algorithm in the Apache Spark framework is proposed in order to find patterns from a big time series. Results corresponding to the electricity consumptions for years 2011, 2012 and 2013 for two buildings from a public university are presented and discussed. Finally, the performance of the proposed methodology in relation to the computational time is compared with that of Weka as benchmarking.},
   author = {R. Perez-Chacon and R. L. Talavera-Llames and F. Martinez-Alvarez and A. Troncoso},
   doi = {10.1007/978-3-319-40162-1_25},
   issn = {21945357},
   journal = {Advances in Intelligent Systems and Computing},
   title = {Finding electric energy consumption patterns in big time series data},
   volume = {474},
   year = {2016},
}
@inproceedings{4,
   abstract = {In this paper, we consider the task of predicting the electricity power generated by photovoltaic solar systems for the next day at half-hourly intervals. We introduce DL, a deep learning approach based on feed-forward neural networks for big data time series, which decomposes the forecasting problem into several sub-problems. We conduct a comprehensive evaluation using 2 years of Australian solar data, evaluating accuracy and training time, and comparing the performance of DL with two other advanced methods based on neural networks and pattern sequence similarity. We investigate the use of multiple data sources (solar power and weather data for the previous days, and weather forecast for the next day) and also study the effect of different historical window sizes. The results show that DL produces competitive accuracy results and scales well, and is thus a highly suitable method for big data environments.},
   author = {José F. Torres and Alicia Troncoso and Irena Koprinska and Zheng Wang and Francisco Martínez-Álvarez},
   doi = {10.1111/exsy.12394},
   issn = {14680394},
   issue = {4},
   journal = {Expert Systems},
   title = {Big data solar power forecasting based on deep learning and multiple data sources},
   volume = {36},
   year = {2019},
}
@article{17,
   abstract = {Due to the intermittent nature of solar energy, accurate photovoltaic power predictions are very important for energy integration into existing energy systems. The evolution of deep learning has also opened the possibility to apply neural network models to predict time series, achieving excellent results. In this paper, a five layer CNN-LSTM model is proposed for photovoltaic power predictions using real data from a location in Temixco, Morelos in Mexico. In the proposed hybrid model, the convolutional layer acts like a filter, extracting local features of the data; then the temporal features are extracted by the long short-term memory network. Finally, the performance of the hybrid model with five layers is compared with a single model (a single LSTM), a CNN-LSTM hybrid model with two layers and two well known popular benchmarks. The results also shows that the hybrid neural network model has better prediction effect than the two layer hybrid model, the single prediction model, the Lasso regression or the Ridge regression.},
   author = {Mario Tovar and Miguel Robles and Felipe Rashid},
   doi = {10.3390/en13246512},
   issn = {19961073},
   issue = {24},
   journal = {Energies},
   title = {PV power prediction, using CNN-LSTM hybrid neural network model. Case of study: Temixco-Morelos, México},
   volume = {13},
   year = {2020},
}
@inproceedings{Madondo2018,
   abstract = {In nonlinear dynamical systems, long-term prediction is extremely challenging. Small perturbations in an initial state can grow exponentially in time and result in large differences in a later advanced state-a behavior known as ​ chaos.​ Chaotic systems tend to have sensitive dependence on initial conditions, much like the Butterfly Effect. Recurrent Neural Networks are dynamic and allow for modeling of chaotic behavior. In this paper, we study and investigate the the modeling and prediction abilities of a Long Short-Term Memory (LSTM) recurrent neural network in dynamical systems with chaotic behavior. In particular, we explore the Lorenz System-which comprises of a nonlinear system of differential equations describing two-dimensional flow of a fluid, and describe an architecture that models the systems' behavior.},
   author = {Malvern Madondo and Thomas Gibbons},
   journal = {MICS 2018 Proceedings},
   title = {Learning and Modeling Chaos Using LSTM Recurrent Neural Networks},
   year = {2018},
}
@article{14,
   abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f(x) = x · sigmoid(βx), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
   author = {Barret Zoph and Quoc V Le},
   issue = {1},
   journal = {6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings},
   title = {SWISH: A SELF-GATED ACTIVATION FUNCTION},
   year = {2018},
}
@article{3,
   abstract = {This paper introduces a novel algorithm for big data time series forecasting. Its main novelty lies in its ability to deal with multivariate data, i.e. to consider multiple time series simultaneously, in order to make multi-output predictions. Real-world processes are typically characterised by several interrelated variables, and the future occurrence of certain time series cannot be explained without understanding the influence that other time series might have on the target time series. One key issue in the context of the multivariate analysis is to determine a priori whether exogenous variables must be included in the model or not. To deal with this, a correlation analysis is used to find a minimum correlation threshold that an exogenous time series must exhibit, in order to be beneficial. Furthermore, the proposed approach has been specifically designed to be used in the context of big data, thus making it possible to efficiently process very large time series. To evaluate the performance of the proposed approach we use data from Spanish electricity prices. Results have been compared to other multivariate approaches showing remarkable improvements both in terms of accuracy and execution time.},
   author = {R. Talavera-Llames and R. Pérez-Chacón and A. Troncoso and F. Martínez-Álvarez},
   doi = {10.1016/j.neucom.2018.07.092},
   issn = {18728286},
   journal = {Neurocomputing},
   title = {MV-kWNN: A novel multivariate and multi-output weighted nearest neighbours algorithm for big data time series forecasting},
   volume = {353},
   year = {2019},
}
@article{5,
   abstract = {It is a well-known fact that improving forecasting accuracy is an important yet often challenging issue. Extensive research has been conducted using neural networks (NNs) to improve their forecasting accuracy. In general, the inputs to NNs are the auto-regressive (i.e. lagged variables) of one or more time series. In addition, either network outputs or network errors have been used as extra inputs to NNs. In this paper, however, we propose a novel recurrent neural network forecasting model which is called the ridge polynomial neural network with error-output feedbacks (RPNN-EOF). RPNN-EOF has two main types of inputs: auto-regressive and moving-average inputs. The former is represented by the lagged variables of a time series, while the latter is represented by feeding back network error to the input layer. In addition, network output is fed back to the input layer. The proposed recurrent model has the ability to produce more accurate forecasts due to the advantages of learning temporal dependence and the direct modelling of the moving-average component. A comparative analysis of RPNN-EOF with five neural network models was completed using ten time series. Simulation results have shown that RPNN-EOF is the most accurate model among all the compared models with the time series used. This shows that employing auto-regressive and moving-average inputs together helps to produce more accurate forecasts.},
   author = {Waddah Waheeb and Rozaida Ghazali},
   doi = {10.1007/s00521-019-04474-5},
   issn = {14333058},
   issue = {13},
   journal = {Neural Computing and Applications},
   title = {A novel error-output recurrent neural network model for time series forecasting},
   volume = {32},
   year = {2020},
}
@article{6,
   abstract = {Energy systems are dynamic and transitional because of alternative energy resources, technological innovations, demand, costs, and environmental consequences. The fossil fuels are the sources of traditional energy generation but has been gradually transitioned to the current innovative technologies with an emphasis on renewable resources like solar, and wind. Despite consistent increases in energy prices, the customers’ demands are escalating rapidly due to an increase in populations, economic development, per capita consumption, supply at remote places, and in static forms for machines and portable devices. The energy storage may allow flexible generation and delivery of stable electricity for meeting demands of customers. The requirements for energy storage will become triple of the present values by 2030 for which very special devices and systems are required. The objective of the current review research is to compare and evaluate the devices and systems presently in use and anticipated for the future. The economic and environmental issues as well as challenges and limitations have been elaborated through deep and strong consultation of literature, previous research, reports and journal. The technologies like flow batteries, super capacitors, SMES (Superconducting magnetic energy storage), FES (Flywheel Energy Storage), PHS (Pumped hydro storage), TES (Thermal Energy Storage), CAES (Compressed Air Energy Storage), and HES (Hybrid energy storage) have been discussed. This article may contribute to guide the decision-makers and the practitioners if they want to select the most recent and innovative devices and systems of energy storage for their grids and other associated uses like machines and portable devices. The characteristics, advantages, limitations, costs, and environmental considerations have been compared with the help of tables and demonstrations to ease their final decision and managing the emerging issues. Thus, the outcomes of this review study may prove highly useful for various stakeholders of the energy sector.},
   author = {Ahmed Zayed AL Shaqsi and Kamaruzzaman Sopian and Amr Al-Hinai},
   doi = {10.1016/J.EGYR.2020.07.028},
   issn = {2352-4847},
   journal = {Energy Reports},
   keywords = {Energy storage,Energy sustainability,Energy transition,Renewable energy},
   month = {12},
   pages = {288-306},
   publisher = {Elsevier},
   title = {Review of energy storage services, applications, limitations, and benefits},
   volume = {6},
   year = {2020},
}
@report{7,
   author = {RAE},
   city = {London},
   institution = {Royal Academy of Engineering},
   title = {Counting the cost: the economic and social costs of electricity shortfalls in the UK},
   year = {2014},
}
@newspaper_article{8,
   author = {Forbes},
   city = {London},
   journal = {Forbes Advisor},
   month = {7},
   title = {Energy Update: Energy Analysts Forecast Even Larger Bills This Winter},
   year = {2022},
}
@article{9,
   abstract = {The number of countries announcing pledges to achieve net-zero emissions over the coming decades continues to grow. But the pledges by governments to date – even if fully achieved – fall well short of what is required to bring global energy-related carbon dioxide emissions to net zero by 2050 and give the world an even chance of limiting the global temperature rise to 1.5 °C. This special report is the world’s first comprehensive study of how to transition to a net zero energy system by 2050 while ensuring stable and affordable energy supplies, providing universal energy access, and enabling robust economic growth. It sets out a cost-effective and economically productive pathway, resulting in a clean, dynamic and resilient energy economy dominated by renewables like solar and wind instead of fossil fuels. The report also examines key uncertainties, such as the roles of bioenergy, carbon capture and behavioural changes in reaching net zero.},
   author = {IEA},
   journal = {Golbal Status Report},
   title = {Net Zero by 2050 A Roadmap for the},
   year = {2021},
}
@newspaper_article{10,
   author = {J Jolly},
   city = {London},
   journal = {Guardian},
   month = {10},
   title = {UK battery ‘gigafactory’ plans huge expansion as electric car demand soars},
   year = {2021},
}
@book{11,
   abstract = {A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines. Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.},
   author = {Kevin P. Murphy},
   journal = {Gaussian processes for machine learning},
   title = {Probabilistic Machine Learning},
   year = {2022},
}
@book{12,
   author = {Edward Raff},
   city = {New York},
   edition = {1},
   pages = {179-184},
   publisher = {Manning Publications Co.},
   title = {Inside Deep Learning - Math, Algorithms, Models},
   year = {2022},
}
@inproceedings{13,
   abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
   author = {Diederik P. Kingma and Jimmy Lei Ba},
   journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
   title = {Adam: A method for stochastic optimization},
   year = {2015},
}
@inproceedings{14,
   abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section. Copyright 2013 by the author(s).},
   author = {Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},
   issue = {PART 3},
   journal = {30th International Conference on Machine Learning, ICML 2013},
   title = {On the difficulty of training recurrent neural networks},
   year = {2013},
}
@inproceedings{ICLR,
   abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f(x) = x · sigmoid(βx), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
   editor = {ICLR},
   journal = {6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings},
   title = {Searching for activation functions},
   year = {2018},
}
@article{16,
   abstract = {Buildings are considered to be one of the world's largest consumers of energy. The productive utilization of energy will spare the accessible energy assets for the following ages. In this paper, we analyze and predict the domestic electric power consumption of a single residential building, implementing deep learning approach (LSTM and CNN). In these models, a novel feature is proposed, the "best N window size"that will focus on identifying the reliable time period in the past data, which yields an optimal prediction model for domestic energy consumption known as deep learning recurrent neural network prediction system with improved sliding window algorithm. The proposed prediction system is tuned to achieve high accuracy based on various hyperparameters. This work performs a comparative study of different variations of the deep learning model and records the best Root Mean Square Error value compared to other learning models for the benchmark energy consumption dataset.},
   author = {Dimpal Tomar and Pradeep Tomar and Arpit Bhardwaj and G. R. Sinha},
   doi = {10.1155/2022/7216959},
   issn = {16875273},
   journal = {Computational Intelligence and Neuroscience},
   title = {Deep Learning Neural Network Prediction System Enhanced with Best Window Size in Sliding Window Algorithm for Predicting Domestic Power Consumption in a Residential Building},
   volume = {2022},
   year = {2022},
}
@article{16,
   abstract = {[PDF]},
   author = {Jeffrey L. Elman},
   doi = {10.1207/s15516709cog1402_1},
   issue = {2},
   journal = {Cognitive Science},
   title = {Finding Structure in Time},
   volume = {14},
   year = {1990},
}
@web_page{17,
   abstract = {A validation dataset is a sample of data held back from training your model that is used to give an estimate of model skill while tuning model’s hyperparameters. The validation dataset is different from the test dataset that is also held back from the training of the model, but is instead used to give an unbiased estimate of the skill of the final tuned model when comparing or selecting between final models. There is much confusion in applied machine learning about what a validation dataset is exactly and how it differs from a test dataset. In this post, you will discover clear definitions for train, test, and validation datasets and how to use each in your own machine learning projects.},
   author = {Jason Brownlee},
   journal = {2021-05-19},
   title = {What is the Difference Between Test and Validation Datasets?},
   year = {2017},
}
@article{18,
   abstract = {In the hydrological cycle, rainfall is a major component and plays a vital role in planning and managing water resources. In this study, new generation deep learning models, recurrent neural network (RNN) and long short-term memory (LSTM), were applied for forecasting monthly rainfall, using long sequential raw data for time series analysis. “All-India” monthly average precipitation data for the period 1871–2016 were taken to build the models and they were tested on different homogeneous regions of India to check their robustness. From the results, it is evident that both the trained models (RNN and LSTM) performed well for different homogeneous regions of India based on the raw data. The study shows that a deep learning network can be applied successfully for time series analysis in the field of hydrology and allied fields to mitigate the risks of climatic extremes.},
   author = {Deepak Kumar and Anshuman Singh and Pijush Samui and Rishi Kumar Jha},
   doi = {10.1080/02626667.2019.1595624},
   issn = {21503435},
   issue = {6},
   journal = {Hydrological Sciences Journal},
   title = {Forecasting monthly precipitation using sequential modelling},
   volume = {64},
   year = {2019},
}
@article{18,
   abstract = {Just as cars and trucks go digital, a scarcity of semiconductors is causing billions of dollars in lost revenue for the automotive industry. Here’s why it’s happening and how to move forward.},
   author = {Ondrej Burkacky and Stephanie Lingemann and Klaus Potoyzky},
   issue = {May},
   journal = {McKinsey Insights},
   title = {Coping with the auto-semiconductor shortage: Strategies for success},
   year = {2021},
}
@article{19,
   abstract = {Accurate prediction of data center resource utilization is required for capacity planning, job scheduling, energy saving, workload placement, and load balancing to utilize the resources efficiently. However, accurately predicting those resources is challenging due to dynamic workloads, heterogeneous infrastructures, and multi-tenant co-hosted applications. Existing prediction methods use fixed size observation windows which cannot produce accurate results because of not being adaptively adjusted to capture local trends in the most recent data. Therefore, those methods train on large fixed sliding windows using an irrelevant large number of observations yielding to inaccurate estimations or fall for inaccuracy due to degradation of estimations with short windows on quick changing trends. In this paper we propose a deep learning-based adaptive window size selection method, dynamically limiting the sliding window size to capture the trend for the latest resource utilization, then build an estimation model for each trend period. We evaluate the proposed method against multiple baseline and state-of-the-art methods, using real data-center workload data sets. The experimental evaluation shows that the proposed solution outperforms those state-of-the-art approaches and yields 16 to 54% improved prediction accuracy compared to the baseline methods.},
   author = {Shuja ur Rehman Baig and Waheed Iqbal and Josep Lluis Berral and David Carrera},
   doi = {10.1016/j.future.2019.10.026},
   issn = {0167739X},
   journal = {Future Generation Computer Systems},
   title = {Adaptive sliding windows for improved estimation of data center resource utilization},
   volume = {104},
   year = {2020},
}
@article{20,
   abstract = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the decaying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
   author = {Sepp Hochreiter},
   doi = {10.1142/S0218488598000094},
   issn = {02184885},
   issue = {2},
   journal = {International Journal of Uncertainty, Fuzziness and Knowlege-Based Systems},
   title = {The vanishing gradient problem during learning recurrent neural nets and problem solutions},
   volume = {6},
   year = {1998},
}
@article{21,
   abstract = {The bootstrap can be used to assess uncertainty of sample estimates.},
   author = {Anthony Kulesa and Martin Krzywinski and Paul Blainey and Naomi Altman},
   doi = {10.1038/nmeth.3414},
   issn = {1548-7091},
   issue = {6},
   journal = {Nature Methods},
   title = {Sampling distributions and the bootstrap},
   volume = {12},
   year = {2015},
}
@article{22,
   author = {Abdelhak M. Zoubir and D. Robert Iskandler},
   doi = {10.1109/MSP.2007.4286560},
   issn = {10535888},
   issue = {4},
   journal = {IEEE Signal Processing Magazine},
   title = {Bootstrap methods and applications},
   volume = {24},
   year = {2007},
}
@article{23,
   abstract = {Energy consumption has been widely studied in the computer architecture field for decades. While the adoption of energy as a metric in machine learning is emerging, the majority of research is still primarily focused on obtaining high levels of accuracy without any computational constraint. We believe that one of the reasons for this lack of interest is due to their lack of familiarity with approaches to evaluate energy consumption. To address this challenge, we present a review of the different approaches to estimate energy consumption in general and machine learning applications in particular. Our goal is to provide useful guidelines to the machine learning community giving them the fundamental knowledge to use and build specific energy estimation methods for machine learning algorithms. We also present the latest software tools that give energy estimation values, together with two use cases that enhance the study of energy consumption in machine learning.},
   author = {Eva García-Martín and Crefeda Faviola Rodrigues and Graham Riley and Håkan Grahn},
   doi = {10.1016/J.JPDC.2019.07.007},
   issn = {0743-7315},
   journal = {Journal of Parallel and Distributed Computing},
   keywords = {Deep learning,Energy consumption,GreenAI,High performance computing,Machine learning},
   month = {12},
   pages = {75-88},
   publisher = {Academic Press},
   title = {Estimation of energy consumption in machine learning},
   volume = {134},
   year = {2019},
}
@inproceedings{24,
   abstract = {The increasing number of decentralized renewable energy sources together with the grow in overall electricity consumption introduce many new challenges related to dimensioning of grid assets and supply-demand balancing. Approximately 40% of the total energy consumption is used to cover the needs of commercial and office buildings. To improve the design of the energy infrastructure and the efficient deployment of resources, new paradigms have to be thought up. Such new paradigms need automated methods to dynamically predict the energy consumption in buildings. At the same time these methods should be easily expandable to higher levels of aggregation such as neighborhoods and the power grid. Predicting energy consumption for a building is complex due to many influencing factors, such as weather conditions, performance and settings of heating and cooling systems, and the number of people present. In this paper, we investigate a newly developed stochastic model for time series prediction of energy consumption, namely the Conditional Restricted Boltzmann Machine (CRBM), and evaluate its performance in the context of building automation systems. The assessment is made on a real dataset consisting of 7 weeks of hourly resolution electricity consumption collected from a Dutch office building. The results showed that for the energy prediction problem solved here, CRBM outperforms Artificial Neural Networks (ANNs), and Hidden Markov Models (HMMs).},
   author = {Elena Mocanu and Phuong H. Nguyen and Madeleine Gibescu and Wil L. Kling},
   doi = {10.1109/PMAPS.2014.6960635},
   journal = {2014 International Conference on Probabilistic Methods Applied to Power Systems, PMAPS 2014 - Conference Proceedings},
   title = {Comparison of machine learning methods for estimating energy consumption in buildings},
   year = {2014},
}
